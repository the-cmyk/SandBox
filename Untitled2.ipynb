{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN22/PPra544BY4yZR58u9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-cmyk/SandBox/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-KhRVIoq2By"
      },
      "source": [
        "#!wget https://github.com/ultralytics/yolov5/releases/download/v3.1/yolov5x.pt\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8ENKu1mxyIv",
        "outputId": "26aa4c22-da0b-45ea-ea21-fb3c75b0d4a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install -U opencv-python pillow pyyaml tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: opencv-python in /usr/local/lib/python3.6/dist-packages (4.4.0.46)\n",
            "Requirement already up-to-date: pillow in /usr/local/lib/python3.6/dist-packages (8.0.1)\n",
            "Requirement already up-to-date: pyyaml in /usr/local/lib/python3.6/dist-packages (5.3.1)\n",
            "Requirement already up-to-date: tqdm in /usr/local/lib/python3.6/dist-packages (4.51.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmiiUxRnrXEB",
        "outputId": "d2086068-110d-4976-eef3-441b5c66648b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://previews.123rf.com/images/wastesoul/wastesoul1703/wastesoul170300034/73416950-close-up-of-the-lane-of-cars-in-traffic-jam.jpg"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-13 09:00:54--  https://previews.123rf.com/images/wastesoul/wastesoul1703/wastesoul170300034/73416950-close-up-of-the-lane-of-cars-in-traffic-jam.jpg\n",
            "Resolving previews.123rf.com (previews.123rf.com)... 23.76.90.133\n",
            "Connecting to previews.123rf.com (previews.123rf.com)|23.76.90.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 126218 (123K) [image/jpeg]\n",
            "Saving to: ‘73416950-close-up-of-the-lane-of-cars-in-traffic-jam.jpg’\n",
            "\n",
            "\r          73416950-   0%[                    ]       0  --.-KB/s               \r73416950-close-up-o 100%[===================>] 123.26K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-11-13 09:00:54 (14.2 MB/s) - ‘73416950-close-up-of-the-lane-of-cars-in-traffic-jam.jpg’ saved [126218/126218]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuD1lLlOwCJq",
        "outputId": "49e0899c-df35-4ece-810d-5b5b11655d7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5l',pretrained=True).fuse().eval()  # yolov5s.pt\n",
        "model = model.autoshape()  # for autoshaping of PIL/cv2/np inputs and NMS\n",
        "\n",
        "# Images\n",
        "img = Image.open('73416950-close-up-of-the-lane-of-cars-in-traffic-jam.jpg')  # PIL image\n",
        "imgs = [img, img, img]\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    prediction = model(imgs, size=640)  # includes NMS\n",
        "\n",
        "for i, (img, pred) in enumerate(zip(imgs, prediction)):\n",
        "    str = 'Image %g/%g: %gx%g ' % (i + 1, len(imgs), *img.shape[:2])\n",
        "    img = Image.fromarray(img.astype(np.uint8)) if isinstance(img, np.ndarray) else img  # from np\n",
        "    if pred is not None:\n",
        "        for c in pred[:, -1].unique():\n",
        "            n = (pred[:, -1] == c).sum()  # detections per class\n",
        "            str += '%g %ss, ' % (n, model.names[int(c)])  # add to string\n",
        "        for *box, conf, cls in pred:  # xyxy, confidence, class\n",
        "            if conf > 0.6:\n",
        "              label = model.names[int(cls)] if hasattr(model, 'names') else 'class_%g' % cls\n",
        "              # str += '%s %.2f, ' % (label, conf)  # label\n",
        "              ImageDraw.Draw(img).rectangle(box, width=cls)  # plot\n",
        "    img.save('results%g.jpg' % i)  # save\n",
        "    print(str + 'Done.')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      7040  models.common.Focus                     [3, 64, 3]                    \n",
            "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  2                -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  4                -1  1   1627904  models.common.BottleneckCSP             [256, 256, 9]                 \n",
            "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  6                -1  1   6499840  models.common.BottleneckCSP             [512, 512, 9]                 \n",
            "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            "  8                -1  1   2624512  models.common.SPP                       [1024, 1024, [5, 9, 13]]      \n",
            "  9                -1  1  10234880  models.common.BottleneckCSP             [1024, 1024, 3, False]        \n",
            " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1   2823680  models.common.BottleneckCSP             [1024, 512, 3, False]         \n",
            " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1    707328  models.common.BottleneckCSP             [512, 256, 3, False]          \n",
            " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1   2561536  models.common.BottleneckCSP             [512, 512, 3, False]          \n",
            " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1  10234880  models.common.BottleneckCSP             [1024, 1024, 3, False]        \n",
            " 24      [17, 20, 23]  1    457725  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model Summary: 335 layers, 4.78187e+07 parameters, 4.78187e+07 gradients\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fusing layers... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Summary: 236 layers, 4.77901e+07 parameters, 4.77901e+07 gradients\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding autoShape... \n",
            "Image 1/3: 866x1300 11 cars, 2 traffic lights, Done.\n",
            "Image 2/3: 866x1300 11 cars, 2 traffic lights, Done.\n",
            "Image 3/3: 866x1300 11 cars, 2 traffic lights, Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}